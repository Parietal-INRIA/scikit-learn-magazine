\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{authblk}
\usepackage{dblfloatfix}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[numbers,square]{natbib}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{url}
\usepackage{magazine-style}

% Listing settings for Python
\lstset{language=python,
	extendedchars=true,
	xleftmargin = 0pt,
	%frame=single,
	%rulecolor=\color{lightgrey},
        aboveskip = 0.5ex,
        belowskip = 0.6ex,
	basicstyle=\small\sffamily,
	escapebegin={\color{green}},
        keywordstyle=\bfseries\color{blue!50!black},
	%identifierstyle=\sffamily\small,
	commentstyle=\color{green!40!black},
	stringstyle=\rmfamily\color{red!60!black},
	showstringspaces=false,
	tabsize=2,
	breaklines=true,
	%classoffset=1,
        morekeywords={{,},=,:}, %keywordstyle=\color{SkoleYellow},
	%classoffset=0,
	backgroundcolor=\color{green!10},
	fillcolor=\color{green!10},
}


\title{Scikit-learn: machine learning without learning the machinery}

\author{The authors}


\begin{document}
\lstset{language=Python}

\maketitle

\begin{abstract}
Machine learning is a pervasive development at the intersection of
statistics and computer science. While it can benefit many
data-related applications, the technicality of the research literature
and the corresponding algorithms slows down its adoption. Scikit-learn is
a open-source software project that aims to make machine learning
accessible to all, whether it be in academia or in the industry. It
benefits from 
the general-purpose Python language, that enjoys a huge growth in
the scientific world, as well as a striving ecosystem of contributors.
Here we give a quick introduction to scikit-learn as well as to
machine-learning basics.
\end{abstract}

\section{A software project across communities}

\paragraph{Project vision.}
%
Scikit-learn was born from the observation that most standard
machine-learning algorithms were out of reach of the users that could
most benefit from them: researchers --biologists, climate
scientists, experimental physicists-- or developers --for web
services or domain-specific applications.
%
Implementations mostly consisted of scattered piece of code to download
on researchers' web pages, with the notable exception of
statistic-specific environments such as the R language \cite{Rmanual}, or
the Weka java library \cite{hall2009weka}.

Scikit-learn aims to bridge the gap between machine-learning research and
applications by providing a library, and not an environment, in a
general-purpose language, relying on domain-agnostic data structures
\cite{pedregosa2011}. Emphasis is put on quality and ease of use, which
implies focus on installation issues, documentation, and API design
\cite{buitinck2013ecml}. For the project to be usable in real-world
settings, computational performance is also a priority.

\paragraph{The Python data ecosystem.}
%
Machine learning is only a small part of a data-analysis pipeline, and
scikit-learn nicely dovetails into the rich Python ecosystem: scientific
and numeric tools \cite{oliphant2007python,varoquaux2013scipy}, but also
text processing tools, web servers...
%
For its numerical needs, scikit-learn leverages NumPy arrays
\cite{vanderwalt2011} --data structures for efficient numerical
computation--, SciPy --a collection of classic numerical algorithms--,
matplotlib \cite{hunter2007matplotlib} --for scientific plotting-- and
Cython, to generate and call compiled code in a Python-like syntax
\cite{behnel2011cython}.
%
A plethora of Python packages can help users with input or preprocessing
of data, notably Pandas for columnar data \cite{mckinney2012}, scikit-image
for images, and NLTK for text. Finally, the IPython environment
\cite{perez2007ipython} is priceless for interactive work.

\paragraph{Some history.}
%
Scikit-learn started circa 2007 as code from David Cournapeau and
Matthieu Brucher's PhD work, but dwindled down till 2010, year at which the
Parietal team from INRIA adopted the project, hiring a full time
engineer. Basics API were defined and an efficient binding of LibSVM
\cite{chang2011libsvm} gave the project a compelling advantage. In
December 2011, the first international sprint was organized with generous
funding from Google. Today, the project has grown vastly beyond
INRIA into a worldwide open source effort.

\section{A brief introduction to machine learning}

\begin{figure}[b]
    %\hspace*{-.015\linewidth}%
    \includegraphics[width=1.05\linewidth]{wage_data}%

    \caption{Wage as a function of years of work experience and
    education; data from \cite{berndt1991}\label{fig:data}.}
\end{figure}

% Introduce and describe the data
Machine learning is about extracting rules from data, most often with the
goal of making decision on new data \cite{elemstatlearn}. As a simple example, we show on
Fig.\,\ref{fig:data} data about wages in the US, from \cite{berndt1991}:
11 features, including wage, number of years of education and of work
experience, on 534 individuals --samples in machine-learning
jargon. This data
Code downloading the data to reproduce the figures can be found on
\url{https://github.com/scikit-learn/SigMobile-paper}.
%
This data showcases many of the typical difficulties machine learning.
The samples are very irregularly distributed, as most individuals
finished their education after high-school. As a result, they are gapping
holes in the 2D plot education versus work experience, in which any
statistical analysis is extrapolation. The data is very noisy: for a
given education and work experience wages vary widely. This variability
can probably be explained with missing factors, such as sector of
activity, but adding them to the analysis creates a more complex picture,
in 3D or more, rather than 2D, with even more gaps.

On Fig.\,\ref{fig:random_forest}, we use a popular machine-learning
algorithm, Random Forests, to try to predict the wage from the years of
education, or the years of work experience, either as separate features,
or combined. The patchy square appearance of the prediction is due to the
inners of the algorithm, and it illustrates the corresponding
extrapolation mechanism. The predictions fit the data very; maybe too
well, as, on the prediction solely from the years of education (left of
the figure), it is hard to believe in the bump at 5 years. This bump is
probably a classic case of \emph{overfit}: the algorithm is learning its
prediction from noise in the data. As a result, the prediction error on
new data will be significantly different than that measured on the
training data.

\begin{figure}[b]
    %\hspace*{-.015\linewidth}%
    \includegraphics[width=1.05\linewidth]{wage_data_random_forest}%

    \caption{Wage prediction from years of work experience and education,
    using random forests.\label{fig:random_forest}}
\end{figure}

% then with svms, and discuss model complexity and use linear
% models to link to convention statistics. Conclude with the difference
% with 1D and 2D, and introduce the curse of dimensionality.
%overfitting, model complexity

On Fig.\,\ref{fig:linear_svm}, we use another popular machine-learning
algorithm, linear Support Vector Machines (SVMs). Unlike random forests,
it can only learn decisions made with linear functions. It is said that
it has a lower \emph{model complexity}, because the number of parameters
to learn from the data is much smaller. The risk here is to
\emph{underfit}: not fully use the richness of the data, that may not be
following a linear law. The art of machine learning consists in choosing
the right family of algorithms (or models) to describe the data well and
find the sweet spot between under and over fit. As the number of features
describing the data grows, the number of parameters to learn grows, and
thus the risk of overfit increases. This core difficulty is known as the
\emph{curse of dimensionality}.

\begin{figure}[b]
    \hspace*{-.015\linewidth}%
    \includegraphics[width=1.05\linewidth]{wage_data_linear_svm}%

    \caption{Wage prediction from years of work experience and education,
    using a linear SVM.\label{fig:linear_svm}}
\end{figure}

\section{Learning with scikit-learn}

\paragraph{Setting up models and feeding them data.}
%
Models in scikit-learn, called \emph{estimators}, are objects
instantiated with all parameters that describe them, but with data. They
have a {\tt fit} method, that accepts an $(n \times p)$ data matrix,
$\mathbf{X}$ --numpy array or scipy sparse matrix--, where $n$ is the
number of samples, and $p$ the number of features--, and possibly a
quantity to predict, $\mathbf{y}$, also an array, 1D unless the
prediction is \emph{multi-output}, with floats for a regression task, as
in the wages prediction, or integers for classification, that is
predicting for each sample to which class it belongs. The data seldom
come in this form, and they must be adapted to be suitable for
consumption by estimators.

\paragraph{Supervised models: learning to predict.}
%
The goal of supervised models is prediction. The corresponding estimators
have a {\tt predict} method, that takes a data matrix $\mathbf{X}$ and
returns a predicted $\mathbf{y}$.

\begin{figure*}[b]
% To have the float on the bottom, we need this declared on the page
% before where it is going to be printed
\lstinputlisting[lastline=12]{nlp_example.py}
\caption{Simple text-processing code to download movie reviews from
Internet and learn positive versus negative ratings.\label{fig:code}}
\end{figure*}


\paragraph{Model evaluation and parameter selection.}
%
The data used to train a model can be used to measure the prediction
error, as it would be enable to distinguish noise from signal, and thus
overly optimistic. The correct way is to leave out \emph{test} data,
untouched during training. Often a \emph{cross-validation} scheme is
used, where the data is repeatedly split into \emph{train} and
\emph{test} subsets. Scikit-learn provides a complete framework for
cross-validation. Specific iterators are used to describe the subsets and
several functions and objects accept as arguments these iterators to
perform cross-validation internally. For instance the {\tt
cross\_val\_score} function measures the prediction score of an
estimator. Cross-validation can also be used to tuned the meta-parameters
of an estimator, such as the sparsity of a sparse model. For this,
specific estimators, \emph{meta-estimators}, such as the {\tt
GridSearchCV}, take another estimator at construction time, and use
cross-validation internally to set its parameter. When used together with
{\tt cross\_val\_score}, they perform \emph{nested} cross-validation,
\emph{i.e.}~the meta-parameters are set independently of the test data?

\paragraph{Unsupervised models: learning to transform.}
%
Unsupervised learning covers all learning applications in which there is
no clearly identified variable to predict. For instance, in a clustering
application, the task is to group together observations that are similar.
Dimension reduction tries to find simplified representations that capture
well the properties of the data. Novelty detection finds in a new dataset
observations that differ from the data in the train set. As the uses of
unsupervised learning are very diverse, it is not possible to have an API
as uniform as for supervised learning. While some unsupervised
estimators can be used to predict a characteristic of new data, such as
in novelty detection, many are useful to transform data, as in dimension
random. Scikit-learn estimators can have a {\tt transform} method, used
for this purpose. A {\tt Pipeline} estimator can then chain estimators to
form a new estimator that applies transformations before calling the {\tt
fit} or {\tt predict} method of the last object.

\section{In practice: putting scikit-learn to good use}

\paragraph{A simple text-mining example.}
%
Not every dataset comes as a set of numbers. For instance in natural
language processing (NLP), each observation is a text, \emph{i.e.} a
string, that needs to be transformed into the data matrix expected by
estimators. Typical, features used to build decisions indicate the
presence of certain words, and often their frequencies.
In spam filtering, it can very relevant to extract from a document that
it contains the term \textsf{viagra}, and possibly how many times.
In other NLP problems such as finding proper names,
events such as ``next token is the Roman numeral'' may be relevant, as in
``J.P. Kennedy III''. Counting bigrams, or $n$-grams, can be a way of
capturing this information. Reusing the {\tt transform} API, scikit-learn
provide a few \emph{feature extraction} objects, that can turn such
unstructured data into a data matrix.
The $\textbf{X}$ matrix is in these situations a
matrix of counting statistics. It is full of zeros, in the sens that very
often a word is present only in a few documents. For better scalability,
it can be encoded using SciPy's sparse matrices, that many scikit-learn
estimators process efficiently.

On Fig.\,\ref{fig:code} we give an example of fully functioning code to
do sentiment polarity classification for movie reviews: given a review,
it outputs whether the review is positive about the movie it concerns.
The code demonstrates the use of pipelines, and also shows how machine
learning ties into unrelated Python's modules: downloading, unpacking and
learning from a dataset are all done in one programming language.

This script first fetches the hand-labeled movie review dataset from
\cite{pang2004} and unpacks it to disk using standard Python libraries.
It then loads these using scikit-learn data loading functions, makes a
pipeline of a \textsf{tf--idf} feature extractor
\cite{rennie2003tackling} --than turns strings into a sparse matrix based
on word frequencies-- and logistic regression, and finally trains the
model. The parameters for the feature extractor and the LogReg classifier
were set manually.

Does this classifier work?
We can find out by classifying two of our own movie reviews.\footnote{
  The code can be tried interactively by simply copy-pasting it
  into an interactive Python interpreter or IPython \cite{perez2007ipython}.
}

\begin{lstlisting}
>>> clf.predict(["Worst movie I ever saw",
...      "Godzilla, eat your heart out!"])
array([0, 1])
\end{lstlisting}

Here, we asked the model to predict the probability of a review being positive
for a batch of two reviews. It returns the negative probability in the first
column, the positive probability in the second.
As we can see, the first review gets a 99.9\% probability for being negative,
while the second has an 83\% probability of being positive.

Of course, that's just anecdotal evidence. We can get a figure for the
classifier's accuracy by retraining it in a five-fold cross-validation scheme:

\begin{lstlisting}
>>> from sklearn import cross_validation
>>> cross_validation.cross_val_score(clf, data.data, data.target)
array([ 0.91, 0.8825, 0.88, 0.8775, 0.86])
\end{lstlisting}

\paragraph{What about big data?}
%
Partial fit, random projections, HashingVectorizer

\section{Nurturing an open source project}

Goal: enable anybody to contribute, have a controlled process, grow.

Github, tests (link to travis)

Difficulty of getting credit, of rewarding properly the long tail of small
contributors, of finding funding. Problem of brain drain.

Difficulty of project scope (ideally: first cover all of statistical
learning classics \cite{elemstatlearn}) and default parameters

We have found that implementing, even a standard algorithm, really well,
can require a lot of domain knowledge. Thus it is natural that specific
libraries span up to solve it. Some adopt scikit-learn API and standard,
and we hope that scikit-learn has a structural effect on the environment.

Impact on education (documentation)

\bibliography{paper}
\bibliographystyle{plain}


\end{document}
