\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{authblk}
\usepackage{dblfloatfix}
%\usepackage{hyperref}
\usepackage{minted}
\usepackage[numbers,square]{natbib}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{url}
\usepackage{magazine-style}

\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\newminted{python}{frame=single,rulecolor=\color{rulecolor}}


\title{Scikit-learn: machine learning without learning the machinery}

\author{The authors}


\begin{document}

\maketitle

\begin{abstract}
Machine learning is a pervasive development at the intersection of
statistics and computer science. While it could benefit to many
data-related applications, the technicality of the research literature
and the corresponding algorithms slows down its adoption. Scikit-learn is
a open-source software project that aims to make machine learning
accessible to all, whether it be in academia or in the industry. It
fosters the general-purpose Python language, that enjoys a huge growth in
the scientific world, and well as a striving ecosystem of contributors.
Here we give a quick introduction to the scikit-learn as well as to
machine-learning basics.
\end{abstract}

\section{A software project across communities}

\paragraph{Project vision}
%
Bridging the gap between machine learning research and applications

\cite{pedregosa2011}

Impact on education (documentation)

Striving for ease-of-use and quality

Importance of API design \cite{buitinck2013ecml}

\paragraph{The python data ecosystem}
%
In a data analysis pipeline, machine learning is only a tiny bit of the
pipeline, although it is critical.

Great scientific and numeric ecosystem
\cite{oliphant2007python,varoquaux2013scipy}, but also text processing,
webservers

NumPy arrays \cite{vanderwalt2011}, SciPy, matplotlib. Cython
\cite{behnel2011cython}

Pandas for columnar data, scikit-image, NLTK.

IPython for interactive work \cite{perez2007ipython}

\paragraph{Some history of the project}
%
PhD project of David Cournapeau and Matthieu Brucher, Parietal-INRIA
comes along,... 2010 sprint, 

LibSVM a good start
\cite{chang2011libsvm}

\section{A brief introduction to machine learning}

Discuss what we can learn from the wages examples. Introduce concepts of
overfitting, model complexity

\section{Learning with scikit-learn}

\paragraph{The data matrix}
%
Blahblah

\paragraph{Supervised models: learning to predict}
%
Blahblah

\paragraph{Model evaluation and parameter selection}
%
Model evaluation:
cross-validation: the concept, cross-val iterators, and cross\_val\_score

Problem of multiple metrics.

CV objects: GridSearchCV and RandomSearchCV, as well as FooBarCV objects.

\paragraph{Unsupervised models: learning to transform}
%
Unsupervised learning: a variety of usage pattern.

Many things can be seen as Transformers

Introduce the pipeline

\section{In practice}

\paragraph{A simple text-mining example}
%
% Lars: the following text presumes that NumPy, estimators, and the
% X/y convention for supervised learning have already been introduced.

Scikit-learn was designed around NumPy principles and data structures,
so virtually all estimators take arrays of numbers as input
and produce arrays as output.
That doesn't mean that it is limited to traditional number crunching domains,
though.
The library has been successfully employed in text classification
and natural language processing problems,
and utility code is provided to translate the unstructured, symbolic data
encountered in these domains
to the rigid matrix/vector structure that estimators expect.

In the NLP application domain,
features are typically 
where features are typically frequencies or boolean indicators
of very large numbers of rare events, such as
``document contains the term \textsf{v1agr@}'' (spam filtering)
or ``next token matches the Roman numeral regex'' (named-entity recognition).
To ensure scalability in the face of such data,
SciPy's sparse matrix data structures are supported by many estimators.\footnote{
  Support for these is added on a by-need basis,
  because using sparse data structures puts constraints on algorithm design
  and implementation.
}

The following example script is a fully functioning
sentiment polarity classifier for movie reviews:
given a review, it outputs the probability that the review is positive
about the movie it concerns.
The script demonstrates the use of pipelines,
and also shows how machine learning ties into Python's
``batteries included'' philosophy:
downloading, unpacking and learning from a dataset
are all done in one programming language.

% XXX make_pipeline is not in 0.14.1, we should release 0.15 sometime
% The example can be made a bit shorter by skipping the movie_reviews dir;
% the tarball from Pang et al. contain top-level cruft.
\begin{pythoncode}
import os.path
import tarfile
from urllib import urlretrieve
from sklearn.datasets import load\_files
from sklearn.feature\_extraction.text import TfidfVectorizer
from sklearn.linear\_models import LogisticRegression
from sklearn.pipeline import make\_pipeline

temp, \_ = urlretrieve('http://www.cs.cornell.edu/people/pabo/movie-review-data/review\_polarity.tar.gz')
with tarfile.open(temp) as tar:
    tar.extractall(path='movie\_reviews')
data = load\_files(os.path.join('movie\_reviews', 'txt\_sentoken'))

clf = make\_pipeline(TfidfVectorizer(min\_df=2, dtype=float,
                                    sublinear\_tf=True, ngram\_range=(1, 2),
                                    strip\_accents='unicode'),
                     LogisticRegression(random\_state=623, C=5000))
clf.fit(data.data, data.target)
\end{pythoncode}

This script first fetches the hand-labeled movie review dataset from
\cite{pang2004} and unpacks it to disk using standard Python libraries.
It then loads these into memory using the scikit-learn function
\texttt{load\_files}, makes a pipeline of a \textsf{tf--idf} feature extractor
\cite{rennie2003tackling} and logistic regression,
and finally trains the model.
The parameters for the feature extractor and the LogReg classifier
were tuned offline; the random-number generator seed is set by hand
to get reproducible results.

Does this classifier work?
We can find out by classifying some of our own movie reviews.\footnote{
  The code can be tried interactively by simply copy-pasting it
  into an interactive Python interpreter or IPython \cite{perez2007ipython}.
}

\begin{pythoncode}
>>> clf.predict\_proba(["This movie is the worst I ever saw.",
...                    "Shawshank Redemption, eat your heart out!"])
array([[  9.99293000e-01,   7.07000280e-04],
       [  1.68757754e-01,   8.31242246e-01]])
\end{pythoncode}

Here, we asked the model to predict the probability of a review being positive
for a batch of two reviews. It returns the negative probability in the first
column, the positive probability in the second.
As we can see, the first review gets a 99.9\% probability for being negative,
while the second has an 83\% probability of being positive.

Of course, that's just anecdotal evidence. We can get a figure for the
classifier's accuracy by retraining it in a five-fold cross-validation scheme:

\begin{pythoncode}
>>> from sklearn.cross\_validation import cross\_val\_score
>>> score = cross\_val\_score(clf, data.data, data.target, cv=5)
>>> score
array([ 0.91  ,  0.8825,  0.88  ,  0.8775,  0.86  ])
>>> score.mean()
0.88200000000000001
\end{pythoncode}

\paragraph{What about big data?}
%
Partial fit, random projections, HashingVectorizer

\section{Nurturing an open source project}

Goal: enable anybody to contribute, have a controlled process, grow.

Github, tests (link to travis)

Difficulty of getting credit, of rewarding properly the long tail of small
contributors, of finding funding. Problem of brain drain.

Difficulty of project scope (ideally: first cover all of statistical
learning classics \cite{elemstatlearn}) and default parameters

We have found that implementing, even a standard algorithm, really well,
can require a lot of domain knowledge. Thus it is natural that specific
libraries span up to solve it. Some adopt scikit-learn API and standard,
and we hope that scikit-learn has a structural effect on the environment.

\small
\bibliography{paper}
\bibliographystyle{plain}


\end{document}
